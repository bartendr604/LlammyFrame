# =============================================================================
# LLAMMY MODULE 5: MASTER COORDINATOR - THE ENTRY POINT & MAINFRAME
# Master_Coordinator.py
# 
# Complete integration of all Llammy modules into single entry point
# Trade show ready with training mode capability
# =============================================================================

import bpy
import os
import sys
import json
import time
import threading
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
from datetime import datetime

# Add addon directory to Python path
addon_dir = Path(__file__).parent
if str(addon_dir) not in sys.path:
    sys.path.insert(0, str(addon_dir))

bl_info = {
    "name": "Llammy Framework v9.0 - Complete System",
    "author": "JJ McQuade",
    "version": (9, 0, 0),
    "blender": (4, 0, 0),
    "location": "View3D > Sidebar > Llammy",
    "description": "Complete AI-powered 3D content creation framework",
    "category": "Development",
}

print(f"üéØ Llammy Master Coordinator v9.0 initializing from: {addon_dir}")

# =============================================================================
# IMPORT ALL MODULES WITH GRACEFUL FALLBACKS
# =============================================================================

class ModuleManager:
    """Manages loading and availability of all Llammy modules"""
    
    def __init__(self):
        self.modules = {}
        self.module_status = {}
        
    def load_modules(self):
        """Load all Llammy modules with graceful fallbacks"""
        
        # Module 1: Core Learning
        try:
            from .llammy_core_learn import (
                core_learning_engine, update_status, update_metrics, 
                save_learning_data, get_core_status
            )
            self.modules['core_learning'] = core_learning_engine
            self.module_status['core_learning'] = True
            print("‚úÖ Module 1: Core Learning loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è Module 1: Core Learning failed: {e}")
            self.module_status['core_learning'] = False
        
        # Module 2: Auto Debug
        try:
            from .llammy_auto_debug import (
                autonomous_debug_system, execute_with_auto_fix, get_debug_system
            )
            self.modules['auto_debug'] = autonomous_debug_system
            self.module_status['auto_debug'] = True
            print("‚úÖ Module 2: Auto Debug loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è Module 2: Auto Debug failed: {e}")
            self.module_status['auto_debug'] = False
        
        # Module 3: RAG System
        try:
            from .lammy_RAG_system import (
                llammy_rag_system, get_context_for_request, create_enhanced_prompt
            )
            self.modules['rag_system'] = llammy_rag_system
            self.module_status['rag_system'] = True
            print("‚úÖ Module 3: RAG System loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è Module 3: RAG System failed: {e}")
            self.module_status['rag_system'] = False
        
        # Module 4: Dual AI Engine  
        try:
            from .fast_dual_ai_engine import IntegratedDualAI
            self.modules['dual_ai'] = IntegratedDualAI()
            self.module_status['dual_ai'] = True
            print("‚úÖ Module 4: Dual AI Engine loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è Module 4: Dual AI Engine failed: {e}")
            self.module_status['dual_ai'] = False
            # Create fallback engine
            self.modules['dual_ai'] = FallbackEngine()
    
    def get_module(self, name: str):
        """Get a specific module"""
        return self.modules.get(name)
    
    def is_available(self, name: str) -> bool:
        """Check if module is available"""
        return self.module_status.get(name, False)
    
    def get_status_summary(self) -> Dict[str, Any]:
        """Get status of all modules"""
        return {
            'modules_loaded': len([s for s in self.module_status.values() if s]),
            'total_modules': len(self.module_status),
            'module_status': self.module_status.copy()
        }

# =============================================================================
# FALLBACK ENGINE FOR WHEN MODULES MISSING
# =============================================================================

class FallbackEngine:
    """Fallback engine when Module 4 is missing"""
    
    def execute_dual_pipeline(self, user_request: str, **kwargs) -> Dict[str, Any]:
        """Fallback dual AI pipeline"""
        try:
            # Simple fallback - create basic Blender code
            code = self._generate_fallback_code(user_request)
            
            # Execute the code
            exec_globals = {'bpy': bpy, '__builtins__': __builtins__}
            exec(code, exec_globals)
            bpy.context.view_layer.update()
            
            return {
                'success': True,
                'creative_response': f"Fallback creative direction for: {user_request}",
                'technical_response': "Generated using fallback engine",
                'generated_code': code,
                'execution_success': True,
                'processing_time': 1.0,
                'quality_score': 70,
                'features_used': ['Fallback'],
                'modules_integrated': ['Fallback']
            }
        except Exception as e:
            return {
                'success': False,
                'error': f"Fallback engine failed: {e}",
                'processing_time': 1.0
            }
    
    def _generate_fallback_code(self, user_request: str) -> str:
        """Generate basic Blender code"""
        request_lower = user_request.lower()
        
        if "sphere" in request_lower:
            return """import bpy
bpy.ops.object.select_all(action='SELECT')
bpy.ops.object.delete(use_global=False)
bpy.ops.mesh.primitive_uv_sphere_add()
print("Fallback: Created sphere")"""
        else:
            return """import bpy
bpy.ops.object.select_all(action='SELECT')
bpy.ops.object.delete(use_global=False)
bpy.ops.mesh.primitive_cube_add()
cube = bpy.context.active_object
cube.name = "Llammy_Fallback_Cube"
print("Fallback: Created cube")"""

# =============================================================================
# TRAINING MODE SYSTEM
# =============================================================================

class TrainingModeManager:
    """Manages autonomous training mode for overnight data collection"""
    
    def __init__(self, master_coordinator):
        self.master = master_coordinator
        self.training_active = False
        self.training_thread = None
        self.training_prompts = [
            "Create a red cube with smooth materials",
            "Generate a blue sphere with metallic finish", 
            "Make a green cylinder with glass material",
            "Build a yellow cone with emission shader",
            "Create a purple torus with subsurface scattering",
            "Design a wooden chair with fabric cushion",
            "Make a glass table with metal legs",
            "Create a ceramic vase with flower pattern",
            "Build a leather sofa with wooden frame",
            "Generate a marble statue with bronze base"
        ]
        self.current_prompt_index = 0
    
    def start_training_mode(self):
        """Start autonomous training mode"""
        if self.training_active:
            return False, "Training mode already active"
        
        self.training_active = True
        self.training_thread = threading.Thread(target=self._training_loop, daemon=True)
        self.training_thread.start()
        
        print("ü§ñ Training mode started - will run overnight")
        return True, "Training mode activated"
    
    def stop_training_mode(self):
        """Stop training mode"""
        self.training_active = False
        if self.training_thread:
            self.training_thread.join(timeout=5)
        
        print("üõë Training mode stopped")
        return True, "Training mode deactivated"
    
    def _training_loop(self):
        """Main training loop - runs autonomously"""
        while self.training_active:
            try:
                # Get next training prompt
                prompt = self.training_prompts[self.current_prompt_index]
                self.current_prompt_index = (self.current_prompt_index + 1) % len(self.training_prompts)
                
                print(f"ü§ñ Training: {prompt}")
                
                # Execute with full pipeline
                result = self.master.execute_pipeline(
                    user_input=prompt,
                    backend='ollama',
                    creative_model='llama3.2:latest',
                    technical_model='llama3.2:latest'
                )
                
                # Log results
                if result.get('success'):
                    print(f"‚úÖ Training success: {prompt}")
                else:
                    print(f"‚ùå Training failed: {prompt} - {result.get('error', 'Unknown')}")
                
                # Wait between training runs (5 minutes)
                time.sleep(300)
                
            except Exception as e:
                print(f"üö® Training loop error: {e}")
                time.sleep(60)  # Wait 1 minute on error
    
    def get_training_status(self) -> Dict[str, Any]:
        """Get training mode status"""
        return {
            'active': self.training_active,
            'current_prompt': self.training_prompts[self.current_prompt_index] if self.training_active else None,
            'total_prompts': len(self.training_prompts),
            'prompt_index': self.current_prompt_index
        }

# =============================================================================
# MASTER COORDINATOR - THE MAINFRAME
# =============================================================================

class LlammyMasterCoordinator:
    """The Master Coordinator - Single entry point for all Llammy functionality"""
    
    def __init__(self):
        self.addon_dir = addon_dir
        self.initialized = False
        self.version = "9.0"
        self.startup_time = time.time()
        
        # Initialize module manager
        self.module_manager = ModuleManager()
        self.module_manager.load_modules()
        
        # Initialize training mode
        self.training_manager = TrainingModeManager(self)
        
        # Status tracking
        self.current_operation = "idle"
        self.last_result = {}
        
        print("üß† Master Coordinator initialized")
    
    def initialize_framework(self) -> Tuple[bool, str]:
        """Initialize the complete Llammy framework"""
        try:
            print("üöÄ Initializing Llammy Framework v9.0...")
            
            # Initialize core learning if available
            if self.module_manager.is_available('core_learning'):
                core_learning = self.module_manager.get_module('core_learning')
                if not core_learning.initialized:
                    core_learning.initialize()
            
            # Initialize RAG system if available
            if self.module_manager.is_available('rag_system'):
                rag_system = self.module_manager.get_module('rag_system')
                try:
                    rag_system.initialize_rag()
                except:
                    print("‚ö†Ô∏è RAG initialization failed - using fallback")
            
            # Set up auto debug with AI models if available
            if self.module_manager.is_available('auto_debug'):
                debug_system = self.module_manager.get_module('auto_debug')
                debug_system.learning_enabled = True
            
            self.initialized = True
            total_time = time.time() - self.startup_time
            
            status = self.module_manager.get_status_summary()
            message = f"Framework ready! {status['modules_loaded']}/{status['total_modules']} modules active in {total_time:.1f}s"
            
            print(f"‚úÖ {message}")
            return True, message
            
        except Exception as e:
            error_msg = f"Framework initialization failed: {e}"
            print(f"‚ùå {error_msg}")
            return False, error_msg
    
    def execute_pipeline(self, user_input: str, backend: str = 'ollama',
                        creative_model: str = '', technical_model: str = '',
                        api_key: str = '') -> Dict[str, Any]:
        """Execute the complete AI pipeline"""
        
        if not user_input.strip():
            return {'success': False, 'error': 'No input provided'}
        
        print(f"üé≠ Executing pipeline: {user_input[:50]}...")
        self.current_operation = "processing"
        
        try:
            # Update status if core learning available
            if self.module_manager.is_available('core_learning'):
                from .llammy_core_learn import update_status
                update_status("executing", "Pipeline processing")
            
            # Execute with dual AI engine
            dual_ai = self.module_manager.get_module('dual_ai')
            result = dual_ai.execute_dual_pipeline(
                user_request=user_input,
                backend=backend,
                creative_model=creative_model,
                technical_model=technical_model,
                api_key=api_key
            )
            
            # Store result
            self.last_result = result
            
            # Update metrics if available
            if self.module_manager.is_available('core_learning'):
                from .llammy_core_learn import update_metrics, save_learning_data
                update_metrics(result.get('success', False), result.get('processing_time', 0))
                save_learning_data(
                    user_input,
                    result.get('generated_code', ''),
                    result.get('success', False),
                    f"v9.0 - {', '.join(result.get('modules_integrated', []))}"
                )
                update_status("idle", "Pipeline completed")
            
            self.current_operation = "idle"
            return result
            
        except Exception as e:
            print(f"‚ùå Pipeline execution failed: {e}")
            self.current_operation = "idle"
            
            if self.module_manager.is_available('core_learning'):
                from .llammy_core_learn import update_status, update_metrics
                update_metrics(False, 1.0)
                update_status("idle", "Pipeline failed")
            
            return {
                'success': False,
                'error': str(e),
                'processing_time': 1.0
            }
    
    def process_file_input(self, file_path: str) -> Dict[str, Any]:
        """Process file input (images, .blend files, etc.)"""
        try:
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                # Image file - use for reference
                user_input = f"Create 3D content based on the uploaded image: {file_path}"
                return self.execute_pipeline(user_input)
            
            elif file_ext == '.blend':
                # Blender file - analyze and enhance
                user_input = f"Analyze and enhance the Blender file: {file_path}"
                return self.execute_pipeline(user_input)
            
            elif file_ext == '.txt':
                # Text file - read content as input
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return self.execute_pipeline(content)
            
            else:
                return {
                    'success': False,
                    'error': f"Unsupported file type: {file_ext}"
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f"File processing failed: {e}"
            }
    
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """Get comprehensive status of entire framework"""
        status = {
            'framework': {
                'version': self.version,
                'initialized': self.initialized,
                'current_operation': self.current_operation,
                'uptime_seconds': time.time() - self.startup_time
            },
            'modules': self.module_manager.get_status_summary(),
            'training': self.training_manager.get_training_status(),
            'last_result': {
                'success': self.last_result.get('success', False),
                'processing_time': self.last_result.get('processing_time', 0),
                'modules_used': self.last_result.get('modules_integrated', [])
            }
        }
        
        # Add core learning metrics if available
        if self.module_manager.is_available('core_learning'):
            core_learning = self.module_manager.get_module('core_learning')
            status['metrics'] = core_learning.get_comprehensive_status()
        
        return status

# =============================================================================
# BLENDER UI INTEGRATION
# =============================================================================

class LLAMMY_PT_MasterPanel(bpy.types.Panel):
    """Master Llammy Control Panel"""
    bl_label = "üé≠ Llammy Framework v9.0"
    bl_idname = "LLAMMY_PT_master_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    
    def draw(self, context):
        layout = self.layout
        scene = context.scene
        
        # Framework status
        status_box = layout.box()
        status_box.label(text="üß† LLAMMY FRAMEWORK v9.0", icon='BLENDER')
        
        if coordinator.initialized:
            status_box.label(text="üü¢ ONLINE", icon='CHECKMARK')
            
            # Module status
            module_status = coordinator.module_manager.get_status_summary()
            status_box.label(text=f"Modules: {module_status['modules_loaded']}/{module_status['total_modules']}")
            
            # Training mode status
            training_status = coordinator.training_manager.get_training_status()
            if training_status['active']:
                status_box.label(text="ü§ñ TRAINING MODE ACTIVE", icon='PLAY')
            
        else:
            status_box.alert = True
            status_box.label(text="üî¥ OFFLINE", icon='ERROR')
            status_box.operator("llammy.initialize_framework", text="üöÄ Initialize Framework")
            return
        
        # Main controls
        controls_box = layout.box()
        controls_box.label(text="üéÆ CONTROLS", icon='TOOL_SETTINGS')
        
        # User input
        controls_box.prop(scene, "llammy_user_input", text="Request")
        
        # Model selection
        model_row = controls_box.row()
        model_row.prop(scene, "llammy_creative_model", text="Creative")
        model_row.prop(scene, "llammy_technical_model", text="Technical")
        
        # Backend selection
        controls_box.prop(scene, "llammy_backend", text="Backend")
        if scene.llammy_backend != 'ollama':
            controls_box.prop(scene, "llammy_api_key", text="API Key")
        
        # Main execute button
        execute_row = controls_box.row()
        execute_row.scale_y = 2.5
        execute_row.operator("llammy.execute_pipeline", text="üöÄ CREATE WITH AI", icon='PLAY')
        
        # File operations
        file_box = layout.box()
        file_box.label(text="üìÅ FILE OPERATIONS", icon='FILE')
        file_box.operator("llammy.process_file", text="üìÇ Process File")
        
        # Training mode controls
        training_box = layout.box()
        training_box.label(text="ü§ñ TRAINING MODE", icon='MOD_BUILD')
        
        training_status = coordinator.training_manager.get_training_status()
        if training_status['active']:
            training_box.label(text=f"Active: {training_status['current_prompt'][:30]}...")
            training_box.operator("llammy.stop_training", text="üõë Stop Training")
        else:
            training_box.operator("llammy.start_training", text="ü§ñ Start Training Mode")
        
        # Utility buttons
        utils_row = controls_box.row()
        utils_row.operator("llammy.system_status", text="üìä Status")
        utils_row.operator("llammy.clear_scene", text="üßπ Clear")
        
        # Results display
        if hasattr(scene, 'llammy_creative_response') and scene.llammy_creative_response:
            result_box = layout.box()
            result_box.label(text="üé® CREATIVE VISION", icon='LIGHT_SUN')
            
            # Show truncated response
            response_lines = scene.llammy_creative_response.split('\n')[:3]
            for line in response_lines:
                if line.strip():
                    display_line = line[:50] + "..." if len(line) > 50 else line
                    result_box.label(text=display_line)
        
        if hasattr(scene, 'llammy_last_result') and scene.llammy_last_result:
            tech_box = layout.box()
            tech_box.label(text="‚öôÔ∏è LAST RESULT", icon='SCRIPT')
            
            try:
                result_data = json.loads(scene.llammy_last_result)
                if result_data.get('success'):
                    tech_box.label(text=f"‚úÖ Success in {result_data.get('processing_time', 0):.1f}s")
                    modules_used = result_data.get('modules_integrated', [])
                    if modules_used:
                        tech_box.label(text=f"Modules: {', '.join(modules_used)}")
                else:
                    tech_box.label(text="‚ùå Failed")
            except:
                tech_box.label(text="Result data available")

# =============================================================================
# BLENDER OPERATORS
# =============================================================================

class LLAMMY_OT_InitializeFramework(bpy.types.Operator):
    """Initialize the Llammy framework"""
    bl_idname = "llammy.initialize_framework"
    bl_label = "Initialize Framework"
    bl_description = "Initialize all Llammy framework systems"
    
    def execute(self, context):
        success, message = coordinator.initialize_framework()
        
        if success:
            self.report({'INFO'}, f"‚úÖ {message}")
        else:
            self.report({'ERROR'}, f"‚ùå {message}")
        
        return {'FINISHED'}

class LLAMMY_OT_ExecutePipeline(bpy.types.Operator):
    """Execute the AI pipeline"""
    bl_idname = "llammy.execute_pipeline"
    bl_label = "Execute AI Pipeline"
    bl_description = "Execute the complete dual AI pipeline"
    
    def execute(self, context):
        scene = context.scene
        
        user_input = getattr(scene, 'llammy_user_input', '').strip()
        if not user_input:
            self.report({'WARNING'}, "Please enter a request")
            return {'CANCELLED'}
        
        self.report({'INFO'}, f"üé¨ Processing: {user_input[:30]}...")
        
        result = coordinator.execute_pipeline(
            user_input=user_input,
            backend=getattr(scene, 'llammy_backend', 'ollama'),
            creative_model=getattr(scene, 'llammy_creative_model', 'llama3.2:latest'),
            technical_model=getattr(scene, 'llammy_technical_model', 'llama3.2:latest'),
            api_key=getattr(scene, 'llammy_api_key', '')
        )
        
        # Store results
        scene.llammy_creative_response = result.get('creative_response', '')[:500]
        scene.llammy_last_result = json.dumps(result)
        
        if result.get('success'):
            score = result.get('quality_score', 0)
            time_taken = result.get('processing_time', 0)
            modules = result.get('modules_integrated', [])
            
            self.report({'INFO'}, f"üéâ Success! Score: {score}% in {time_taken:.1f}s | {', '.join(modules)}")
        else:
            self.report({'ERROR'}, f"‚ùå Failed: {result.get('error', 'Unknown error')}")
        
        return {'FINISHED'}

class LLAMMY_OT_StartTraining(bpy.types.Operator):
    """Start training mode"""
    bl_idname = "llammy.start_training"
    bl_label = "Start Training Mode"
    bl_description = "Start autonomous training mode for overnight data collection"
    
    def execute(self, context):
        success, message = coordinator.training_manager.start_training_mode()
        
        if success:
            self.report({'INFO'}, f"ü§ñ {message}")
        else:
            self.report({'WARNING'}, f"‚ö†Ô∏è {message}")
        
        return {'FINISHED'}

class LLAMMY_OT_StopTraining(bpy.types.Operator):
    """Stop training mode"""
    bl_idname = "llammy.stop_training"
    bl_label = "Stop Training Mode"
    bl_description = "Stop autonomous training mode"
    
    def execute(self, context):
        success, message = coordinator.training_manager.stop_training_mode()
        
        if success:
            self.report({'INFO'}, f"üõë {message}")
        else:
            self.report({'WARNING'}, f"‚ö†Ô∏è {message}")
        
        return {'FINISHED'}

class LLAMMY_OT_ProcessFile(bpy.types.Operator):
    """Process uploaded file"""
    bl_idname = "llammy.process_file"
    bl_label = "Process File"
    bl_description = "Process uploaded file (image, .blend, .txt)"
    
    filepath: bpy.props.StringProperty(subtype="FILE_PATH")
    filter_image: bpy.props.BoolProperty(default=True, options={'HIDDEN'})
    filter_text: bpy.props.BoolProperty(default=True, options={'HIDDEN'})
    filter_blend: bpy.props.BoolProperty(default=True, options={'HIDDEN'})
    
    def execute(self, context):
        if not self.filepath:
            self.report({'WARNING'}, "No file selected")
            return {'CANCELLED'}
        
        self.report({'INFO'}, f"üìÇ Processing file: {Path(self.filepath).name}")
        
        result = coordinator.process_file_input(self.filepath)
        
        if result.get('success'):
            self.report({'INFO'}, "‚úÖ File processed successfully")
        else:
            self.report({'ERROR'}, f"‚ùå File processing failed: {result.get('error', 'Unknown')}")
        
        return {'FINISHED'}
    
    def invoke(self, context, event):
        context.window_manager.fileselect_add(self)
        return {'RUNNING_MODAL'}

class LLAMMY_OT_SystemStatus(bpy.types.Operator):
    """Show system status"""
    bl_idname = "llammy.system_status"
    bl_label = "System Status"
    bl_description = "Show comprehensive system status"
    
    def execute(self, context):
        status = coordinator.get_comprehensive_status()
        
        modules_active = status['modules']['modules_loaded']
        modules_total = status['modules']['total_modules']
        uptime = status['framework']['uptime_seconds']
        
        self.report({'INFO'}, f"üìä Framework: {modules_active}/{modules_total} modules, {uptime:.1f}s uptime")
        
        # Print detailed status to console
        print("\n" + "="*60)
        print("üé≠ LLAMMY FRAMEWORK v9.0 STATUS")
        print("="*60)
        print(f"Framework: {status['framework']}")
        print(f"Modules: {status['modules']}")
        print(f"Training: {status['training']}")
        print(f"Last Result: {status['last_result']}")
        print("="*60)
        
        return {'FINISHED'}

class LLAMMY_OT_ClearScene(bpy.types.Operator):
    """Clear scene"""
    bl_idname = "llammy.clear_scene"
    bl_label = "Clear Scene"
    bl_description = "Clear all objects from scene"
    
    def execute(self, context):
        bpy.ops.object.select_all(action='SELECT')
        bpy.ops.object.delete(use_global=False)
        self.report({'INFO'}, "üßπ Scene cleared")
        return {'FINISHED'}

# =============================================================================
# PROPERTIES AND REGISTRATION
# =============================================================================

def register_properties():
    """Register Blender scene properties"""
    bpy.types.Scene.llammy_user_input = bpy.props.StringProperty(
        name="Request",
        description="Describe what you want to create",
        default="Create a blue sphere with metallic material",
        maxlen=1000
    )
    
    bpy.types.Scene.llammy_creative_model = bpy.props.StringProperty(
        name="Creative Model",
        description="Model for creative direction",
        default="llama3.2:latest"
    )
    
    bpy.types.Scene.llammy_technical_model = bpy.props.StringProperty(
        name="Technical Model",
        description="Model for code generation",
        default="llama3.2:latest"
    )
    
    bpy.types.Scene.llammy_backend = bpy.props.EnumProperty(
        name="Backend",
        description="AI backend to use",
        items=[
            ('ollama', 'Ollama (Local)', 'Local Ollama models'),
            ('claude', 'Claude API', 'Anthropic Claude'),
            ('gemini', 'Gemini API', 'Google Gemini')
        ],
        default='ollama'
    )
    
    bpy.types.Scene.llammy_api_key = bpy.props.StringProperty(
        name="API Key",
        description="API key for cloud services",
        default="",
        subtype='PASSWORD'
    )
    
    bpy.types.Scene.llammy_creative_response = bpy.props.StringProperty(
        name="Creative Response",
        default=""
    )
    
    bpy.types.Scene.llammy_last_result = bpy.props.StringProperty(
        name="Last Result",
        default=""
    )

def unregister_properties():
    """Unregister Blender scene properties"""
    props = [
        'llammy_user_input', 'llammy_creative_model', 'llammy_technical_model',
        'llammy_backend', 'llammy_api_key', 'llammy_creative_response', 'llammy_last_result'
    ]
    
    for prop in props:
        if hasattr(bpy.types.Scene, prop):
            delattr(bpy.types.Scene, prop)

# =============================================================================
# REGISTRATION
# =============================================================================

# List of classes to register
classes = [
    LLAMMY_PT_MasterPanel,
    LLAMMY_OT_InitializeFramework,
    LLAMMY_OT_ExecutePipeline,
    LLAMMY_OT_StartTraining,
    LLAMMY_OT_StopTraining,
    LLAMMY_OT_ProcessFile,
    LLAMMY_OT_SystemStatus,
    LLAMMY_OT_ClearScene,
]

# Global coordinator instance
coordinator = LlammyMasterCoordinator()

def register():
    """Register the Llammy framework"""
    print("üîÑ Registering Llammy Framework v9.0...")
    
    try:
        # Register all classes
        for cls in classes:
            bpy.utils.register_class(cls)
        
        # Register properties
        register_properties()
        
        # Auto-initialize the framework
        success, message = coordinator.initialize_framework()
        
        if success:
            print(f"üéâ LLAMMY FRAMEWORK v9.0 READY!")
            print(f"üìç Access via: 3D Viewport ‚Üí Sidebar ‚Üí Llammy tab")
            print(f"üéØ {message}")
            
            # Show module status
            status = coordinator.module_manager.get_status_summary()
            for module_name, available in status['module_status'].items():
                icon = "‚úÖ" if available else "‚ùå"
                print(f"{icon} {module_name}: {'Available' if available else 'Missing'}")
                
        else:
            print(f"‚ö†Ô∏è Framework partially initialized: {message}")
        
        print(f"üöÄ Ready for trade show demo!")
        
    except Exception as e:
        print(f"‚ùå Registration failed: {e}")
        import traceback
        traceback.print_exc()

def unregister():
    """Unregister the Llammy framework"""
    print("üîÑ Unregistering Llammy Framework v9.0...")
    
    try:
        # Stop training mode if active
        if coordinator.training_manager.training_active:
            coordinator.training_manager.stop_training_mode()
        
        # Unregister properties
        unregister_properties()
        
        # Unregister classes
        for cls in reversed(classes):
            try:
                bpy.utils.unregister_class(cls)
            except:
                pass
        
        print("‚úÖ Llammy Framework v9.0 unregistered")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Unregister error: {e}")

# =============================================================================
# INSTALLATION INSTRUCTIONS
# =============================================================================

print("üíé MODULE 5: MASTER COORDINATOR v9.0 - COMPLETE!")
print("")
print("üéØ INSTALLATION INSTRUCTIONS:")
print("1. Save this as 'Master_Coordinator.py' in your addon folder")
print("2. Make sure all 4 other modules are in the same folder:")
print("   - llammy_core_learn.py")
print("   - llammy_auto_debug.py") 
print("   - lammy_RAG_system.py")
print("   - fast_dual_ai_engine.py")
print("3. In Blender: Edit ‚Üí Preferences ‚Üí Add-ons ‚Üí Install ‚Üí Select this file")
print("4. Enable the addon")
print("5. Go to 3D Viewport ‚Üí Sidebar ‚Üí Llammy tab")
print("6. Click 'Initialize Framework'")
print("7. Start creating with AI!")
print("")
print("ü§ñ TRAINING MODE:")
print("- Click 'Start Training Mode' before bed")
print("- Let it run overnight collecting training data")
print("- Stop in the morning with fresh dataset")
print("")
print("üìÅ FILE OPERATIONS:")
print("- Upload images for reference")
print("- Process .blend files for enhancement")
print("- Drop .txt files with complex instructions")
print("")
print("üé≠ TRADE SHOW READY!")
print("- Multiple input methods")
print("- Autonomous training")
print("- Complete integration")
print("- Professional UI")
print("- Graceful fallbacks")
print("")
print("üöÄ NOW GO INSTALL IT AND SET IT TO TRAINING MODE!")
print("üò¥ THEN GET SOME SLEEP!")

if __name__ == "__main__":
    register()